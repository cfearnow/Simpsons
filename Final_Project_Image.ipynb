{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc6f785",
   "metadata": {},
   "source": [
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e9daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import skimage as sk\n",
    "from skimage import util, io\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from collections import Counter, deque\n",
    "from moviepy.editor import *\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from scipy import ndarray\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4939a",
   "metadata": {},
   "source": [
    "### Create a dictionary of the characters names that will be included in the analysis.\n",
    "\n",
    "This dictionary will be used to map the character names with the associated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53a3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_characters = {0: 'abraham_grampa_simpson', 1: 'agnes_skinner', 2: 'apu_nahasapeemapetilon', 3: 'barney_gumble', \n",
    "                  4: 'bart_simpson', 5: 'carl_carlson', 6: 'charles_montgomery_burns', 7: 'chief_wiggum', \n",
    "                  8: 'cletus_spuckler', 9: 'comic_book_guy', 10: 'edna_krabappel', 11: 'groundskeeper_willie',\n",
    "                  12: 'homer_simpson', 13: 'kent_brockman', 14: 'krusty_the_clown', 15: 'lenny_leonard', \n",
    "                  16: 'lisa_simpson', 17: 'maggie_simpson', 18: 'marge_simpson', 19: 'martin_prince', 20: 'mayor_quimby',\n",
    "                  21: 'milhouse_van_houten', 22: 'moe_szyslak', 23: 'ned_flanders', 24: 'nelson_muntz', 25: 'otto_mann',\n",
    "                  26: 'patty_bouvier', 27: 'principal_skinner', 28: 'professor_joun_frink', 29: 'rainier_wolf_castle', \n",
    "                  30: 'ralph_wiggum' , 31: 'selma_bouvier', 32: 'sideshow_bob', 33: 'sideshow_mel', 34: 'snake_jailbird', \n",
    "                  35: 'waylon_smithers'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9262d24",
   "metadata": {},
   "source": [
    "### Create all of the directories that will be used in the analysis as well as any fixed variables that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011734ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent directory path.  This directory is the parent directory for the entire project and likely the working directory \n",
    "os.chdir('D:/School_Files/Simpsons_Project')\n",
    "\n",
    "# Directory to master image file\n",
    "image_dir = './simpsons_dataset/' \n",
    "\n",
    "# Directory to augmented image file\n",
    "aug_path = './simpsons_augmented'\n",
    "\n",
    "pic_size = 128 #The size that each image will be modified to\n",
    "batch_size = 32 #The batch size the images will be fed through the model\n",
    "epochs = 50 #The number of epochs that will be run\n",
    "num_classes = len(map_characters) #The number of classes for the analysis (number of characters)\n",
    "pictures_per_class = 1500 #Number of images for each character\n",
    "val_size = 0.15 #Size of the validation set\n",
    "aug_images_number = 2500 #Number of images that will be created after image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12605a",
   "metadata": {},
   "source": [
    "### Create all the required image augmentation functions\n",
    "\n",
    "We do not have a consistent number of images for each character, and in some instances, there are not enough images of a character to conduct the analysis.  In order to adjust for this, rather than going out to obtain additional images, we will use image augmentation to ensure that both, we have a sufficient number of images, and each image is different.  The next several steps will set up out image augmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5ad5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random image rotation between -45 anf 45 deg\n",
    "def random_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "    random_degree = random.randint(-45, 45)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "# Random noise\n",
    "def random_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "# Flip the image on the y axis (horizontally)\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "    return image_array[:, ::-1]\n",
    "\n",
    "# Randomly crop 1 to 75 pixels from each side of the image\n",
    "def crop_image(image_array: ndarray):\n",
    "    rand_1 = random.randint(1,75)\n",
    "    rand_2 = random.randint(1,75)\n",
    "    rand_3 = random.randint(1,75)\n",
    "    rand_4 = random.randint(1,75)\n",
    "    return sk.util.crop(image_array, ((rand_1, rand_2), (rand_3, rand_4), (0,0)), copy=False)\n",
    "\n",
    "# Create a dictionary of the transformations we defined above for use in functions\n",
    "available_transformations = {'rotate': random_rotation,\n",
    "                             'noise': random_noise,\n",
    "                             'horizontal_flip': horizontal_flip,\n",
    "                             'crop': crop_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc37ab6",
   "metadata": {},
   "source": [
    "### Create the function that will do the actual image augmentation. \n",
    "\n",
    "This function not only conducts the image augmentation, it also ensures each of the augmentations are completely random.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(aug_images_number):\n",
    "    # Check if augmented directory exists\n",
    "    folder_check = os.path.isdir(aug_path)\n",
    "        \n",
    "    if not folder_check:\n",
    "        os.makedirs(aug_path)\n",
    "        print(\"created folder : \", aug_path)\n",
    "    else:\n",
    "        print(aug_path, \"already exists.\")\n",
    "        \n",
    "        for k, char in map_characters.items():\n",
    "            folder_path = os.path.join(aug_path, char)             \n",
    "            num_files_desired = aug_images_number            \n",
    "            subfolder_check = os.path.isdir(folder_path)\n",
    "            \n",
    "            if not subfolder_check:\n",
    "                shutil.copytree(os.path.join(image_dir , char), folder_path)\n",
    "                print(\"copied folder : \", folder_path)\n",
    "                \n",
    "                images = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        \n",
    "                num_generated_files = len(os.listdir(os.path.join(image_dir , char)))\n",
    "                while num_generated_files <= num_files_desired:\n",
    "                    # random image from the folder\n",
    "                    image_path = random.choice(images)\n",
    "                    # read image as an two dimensional array of pixels\n",
    "                    image_to_transform = sk.io.imread(image_path)\n",
    "                    # random num of transformation to apply\n",
    "                    num_transformations_to_apply = random.randint(1, len(available_transformations))\n",
    "                \n",
    "                    num_transformations = 0\n",
    "                    transformed_image = None\n",
    "                    while num_transformations <= num_transformations_to_apply:\n",
    "                        # random transformation to apply for a single image\n",
    "                        key = random.choice(list(available_transformations))\n",
    "                        transformed_image = available_transformations[key](image_to_transform)\n",
    "                        num_transformations += 1\n",
    "                \n",
    "                        new_file_path = '%s/pic_%s.jpg' % (folder_path, num_generated_files)\n",
    "                \n",
    "                        # write image to the disk\n",
    "                        io.imsave(new_file_path, transformed_image)\n",
    "                    num_generated_files += 1    \n",
    "                    \n",
    "            elif len(glob.glob(aug_path + '/%s/*' % char)) < aug_images_number:\n",
    "                \n",
    "                print('adding ', (aug_images_number - len(glob.glob(aug_path + '/%s/*' % char))), ' images to ', folder_path)\n",
    "                num_generated_files = len(glob.glob(aug_path + '/%s/*' % char))\n",
    "                images = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "                \n",
    "                while num_generated_files <= num_files_desired:\n",
    "                    # random image from the folder\n",
    "                    image_path = random.choice(images)\n",
    "                    # read image as an two dimensional array of pixels\n",
    "                    image_to_transform = sk.io.imread(image_path)\n",
    "                    # random num of transformation to apply\n",
    "                    num_transformations_to_apply = random.randint(1, len(available_transformations))\n",
    "                \n",
    "                    num_transformations = 0\n",
    "                    transformed_image = None\n",
    "                    while num_transformations <= num_transformations_to_apply:\n",
    "                        # random transformation to apply for a single image\n",
    "                        key = random.choice(list(available_transformations))\n",
    "                        transformed_image = available_transformations[key](image_to_transform)\n",
    "                        num_transformations += 1\n",
    "                \n",
    "                        new_file_path = '%s/pic_%s.jpg' % (folder_path, num_generated_files)\n",
    "                \n",
    "                        # write image to the disk\n",
    "                        io.imsave(new_file_path, transformed_image)\n",
    "                    num_generated_files += 1\n",
    "            else:\n",
    "                print(folder_path, \"already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974575ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pictures(BGR):\n",
    "    \"\"\"\n",
    "    Load pictures from folders for characters from the map_characters dict and create a numpy dataset and \n",
    "    a numpy labels set. Pictures are re-sized into picture_size square.\n",
    "    :param BGR: boolean to use true color for the picture (RGB instead of BGR for plt)\n",
    "    :return: dataset, labels set\n",
    "    \"\"\"\n",
    "    pics = []\n",
    "    labels = []\n",
    "    for k, char in map_characters.items():\n",
    "        pictures = [k for k in glob.glob(aug_path + '/%s/*' % char)]\n",
    "        nb_pic = round(pictures_per_class/(1-test_size)) if round(pictures_per_class/(1-test_size))<len(pictures) else len(pictures)\n",
    "        # nb_pic = len(pictures)\n",
    "        for pic in np.random.choice(pictures, nb_pic):\n",
    "            a = cv2.imread(pic)\n",
    "            if BGR:\n",
    "                a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "            a = cv2.resize(a, (pic_size,pic_size))\n",
    "            pics.append(a)\n",
    "            labels.append(k)\n",
    "    return np.array(pics), np.array(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(save=False, load=False, BGR=False):\n",
    "    \"\"\"\n",
    "    Create the actual dataset split into train and test, pictures content is as float32 and\n",
    "    normalized (/255.). The dataset could be saved or loaded from h5 files.\n",
    "    :param save: saving or not the created dataset\n",
    "    :param load: loading or not the dataset\n",
    "    :param BGR: boolean to use true color for the picture (RGB instead of BGR for plt)\n",
    "    :return: X_train, X_test, y_train, y_test (numpy arrays)\n",
    "    \"\"\"\n",
    "    if load:\n",
    "        h5f = h5py.File('dataset.h5','r')\n",
    "        X_train = h5f['X_train'][:]\n",
    "        X_test = h5f['X_test'][:]\n",
    "        h5f.close()    \n",
    "\n",
    "        h5f = h5py.File('labels.h5','r')\n",
    "        y_train = h5f['y_train'][:]\n",
    "        y_test = h5f['y_test'][:]\n",
    "        h5f.close()    \n",
    "    else:\n",
    "        X, y = load_pictures(BGR)\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        if save:\n",
    "            h5f = h5py.File('dataset.h5', 'w')\n",
    "            h5f.create_dataset('X_train', data=X_train)\n",
    "            h5f.create_dataset('X_test', data=X_test)\n",
    "            h5f.close()\n",
    "\n",
    "            h5f = h5py.File('labels.h5', 'w')\n",
    "            h5f.create_dataset('y_train', data=y_train)\n",
    "            h5f.create_dataset('y_test', data=y_test)\n",
    "            h5f.close()\n",
    "            \n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "    print(\"Train\", X_train.shape, y_train.shape)\n",
    "    print(\"Test\", X_test.shape, y_test.shape)\n",
    "    if not load:\n",
    "        dist = {k:tuple(d[k] for d in [dict(Counter(np.where(y_train==1)[1])), dict(Counter(np.where(y_test==1)[1]))]) \n",
    "                for k in range(num_classes)}\n",
    "        print('\\n'.join([\"%s : %d train pictures & %d test pictures\" % (map_characters[k], v[0], v[1]) \n",
    "            for k,v in sorted(dist.items(), key=lambda x:x[1][0], reverse=True)]))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_six_conv(input_shape):\n",
    "    \"\"\"\n",
    "    CNN Keras model with 6 convolutions.\n",
    "    :param input_shape: input shape, generally X_train.shape[1:]\n",
    "    :return: Keras model, RMS prop optimizer\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same')) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=opt,\n",
    "          metrics=['accuracy'])    \n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fe560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(weights_path, six_conv=False, input_shape=(pic_size,pic_size,3)):\n",
    "    model, opt = create_model_six_conv(input_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53547569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 0.01\n",
    "    return lr*(0.1**int(epoch/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14359234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, X_train, X_test, y_train, y_test, data_augmentation=True):\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    :param model: Keras sequential model\n",
    "    :param data_augmentation: boolean for data_augmentation (default:True)\n",
    "    :param callback: boolean for saving model checkpoints and get the best saved model\n",
    "    :param six_conv: boolean for using the 6 convs model (default:False, so 4 convs)\n",
    "    :return: model and epochs history (acc, loss, val_acc, val_loss for every epoch)\n",
    "    \"\"\"\n",
    "    if data_augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # Compute quantities required for feature-wise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(X_train)\n",
    "        filepath=\"weights_6conv_%s.hdf5\" % time.strftime(\"%d%m/%Y\")\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "        callbacks_list = [LearningRateScheduler(lr_schedule) ,checkpoint]\n",
    "        history = model.fit(datagen.flow(X_train, y_train,\n",
    "                            batch_size=batch_size),\n",
    "                            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            callbacks=callbacks_list)        \n",
    "    else:\n",
    "        history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          shuffle=True)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e24d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    image_augmentation(aug_images_number)\n",
    "    X_train, X_test, y_train, y_test = get_dataset(save=True)\n",
    "    model, opt = create_model_six_conv(X_train.shape[1:])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    model, history = training(model, X_train, X_test, y_train, y_test, data_augmentation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
